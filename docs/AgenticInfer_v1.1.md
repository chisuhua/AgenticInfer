
# **AgenticInferï¼šAgentic-native æ¨ç†å¼•æ“æ¶æ„è®¾è®¡æ–‡æ¡£ v1.2**  

---

## ä¸€ã€è®¾è®¡ç›®æ ‡ä¸æ ¸å¿ƒç†å¿µ

### 1.1 æ ¸å¿ƒç†å¿µï¼ˆä¸¥æ ¼éµå¾ª AgenticDSL v3.7ï¼‰

> **â€œæ¨ç†è¡Œä¸ºåº”æˆä¸ºå¯éªŒè¯ã€å¯ç»„åˆã€å¯å½’æ¡£çš„ DAG èŠ‚ç‚¹ï¼Œè€Œéé»‘ç›’ã€‚â€**

- âœ… **åŸç”Ÿå…¼å®¹ `llm_call`**ï¼šåˆ©ç”¨è§„èŒƒå…è®¸çš„â€œé¢å¤–å­—æ®µâ€æœºåˆ¶ï¼ˆ5.7ï¼‰ï¼Œæ— éœ€æ–°å¢å­—æ®µ  
- âœ… **èƒ½åŠ›å¥‘çº¦åŒ–**ï¼šé€šè¿‡ `/lib/reasoning/**` æ ‡å‡†å­å›¾æš´éœ²æ¨ç†èƒ½åŠ›ï¼ˆ6.2ï¼‰  
- âœ… **èµ„æºå£°æ˜é©±åŠ¨**ï¼šé€šè¿‡ `/__meta__/resources` å£°æ˜ `reasoning` èƒ½åŠ›ï¼ˆ6.4ï¼‰  
- âœ… **å®Œå…¨ä¸‰å±‚æ¶æ„**ï¼šæ‰§è¡ŒåŸè¯­å±‚ (`llm_call`) â†’ æ ‡å‡†åŸè¯­å±‚ (`/lib/reasoning/**`) â†’ çŸ¥è¯†åº”ç”¨å±‚ (`/app/inference/**`)  
- âœ… **å¼•æ“å³åº”ç”¨**ï¼šAgenticInfer æœ¬èº«æ˜¯ä¸€ä¸ª AgenticDSL åº”ç”¨ï¼ˆ`/app/inference/native_engine_v1`ï¼‰

---

## äºŒã€æ•´ä½“æ¶æ„

### 2.1 æ‰§è¡Œæµç¨‹ï¼ˆæ— è¯­ä¹‰æ‰©å±•ï¼‰

```mermaid
flowchart LR
    A[/main/task] --> B[/lib/reasoning/structured_generate@v1]
    B --> C{æ‰§è¡Œå™¨ï¼šæ˜¯å¦å£°æ˜ native_inference_core?}
    C -->|æ˜¯| D[å¯åŠ¨åµŒå¥—å¼•æ“: /app/inference/native_engine_v1]
    C -->|å¦| E[è°ƒç”¨ä¼ ç»Ÿåç«¯ï¼ˆå¦‚ llama.cppï¼‰]
    D --> F[/app/inference/tokenize@v1]
    F --> G[/app/inference/alloc_kv@v1]
    G --> H[/app/inference/run_attention@v1]
    H --> I[/app/inference/apply_grammar@v1]
    I --> J[/app/inference/stream_output@v1]
    J --> K[è¿”å›ä¸»ä¸Šä¸‹æ–‡]
```

### 2.2 è§¦å‘æœºåˆ¶ï¼ˆåˆè§„æ–¹å¼ï¼‰

- ç”¨æˆ·åœ¨ `/__meta__/resources` ä¸­å£°æ˜ï¼š
  ```agentic
  - type: tool
    name: native_inference_core
    scope: internal
  ```
- æ‰§è¡Œå™¨åœ¨ DAG å¯åŠ¨æ—¶æ£€æµ‹è¯¥èµ„æº
- **æ‰€æœ‰ `llm_call` èŠ‚ç‚¹è‡ªåŠ¨è·¯ç”±è‡³ AgenticInfer**
- **æœªå£°æ˜åˆ™é™çº§è‡³ä¼ ç»Ÿåç«¯**

> âœ… **åˆè§„ä¾æ®**ï¼š5.7 å…è®¸ `llm` å¯¹è±¡åŒ…å«é¢å¤–å­—æ®µï¼›6.4 å…è®¸å·¥å…·èµ„æºå£°æ˜

---

## ä¸‰ã€æ ‡å‡†åŸè¯­å±‚ï¼š`/lib/reasoning/**`ï¼ˆå¿…é¡»å®ç°ï¼‰

æ ¹æ® AgenticDSL v3.7 **é™„å½• C** ä¸ **10.2 æ¨ç†åŸè¯­**ï¼Œæ–°å¢ä»¥ä¸‹ 5 ä¸ªæ ‡å‡†å­å›¾ï¼ˆå‡å¸¦ `signature`ï¼‰ï¼š

### 1. **`/lib/reasoning/generate_text@v1`ï¼ˆstableï¼‰**

```yaml
signature:
  inputs:
    - name: prompt; type: string; required: true
    - name: model; type: string; required: true
    - name: seed; type: integer; required: true
    - name: temperature; type: number; default: 0.0
    - name: max_tokens; type: integer; default: 256
  outputs:
    - name: text; type: string
    - name: kv_handle; type: string
version: "1.0"
stability: stable
permissions: [reasoning: lmm_generate]
type: llm_call
llm:
  model: "{{ $.model }}"
  seed: "{{ $.seed }}"
  temperature: "{{ $.temperature }}"
  max_tokens: "{{ $.max_tokens }}"
```

---

### 2. **`/lib/reasoning/structured_generate@v1`ï¼ˆstableï¼‰**

```yaml
signature:
  inputs:
    - name: prompt; type: string; required: true
    - name: output_schema; type: object; required: true
    - name: seed; type: integer; required: true
    - name: model; type: string; required: true
  outputs:
    - name: parsed_output; type: object
version: "1.0"
stability: stable
permissions: [reasoning: structured_generate]
type: llm_call
llm:
  model: "{{ $.model }}"
  seed: "{{ $.seed }}"
  temperature: 0.0
  # output_schema ä½œä¸ºé¢å¤–å­—æ®µï¼Œç”± AgenticInfer è¯†åˆ«
```

---

### 3. **`/lib/reasoning/continue_from_kv@v1`ï¼ˆstableï¼‰**

```yaml
signature:
  inputs:
    - name: kv_handle; type: string; required: true
    - name: new_prompt; type: string; required: true
    - name: model; type: string; required: true
  outputs:
    - name: continuation; type: string
    - name: updated_kv_handle; type: string
version: "1.0"
stability: stable
permissions: [reasoning: lmm_generate]
type: llm_call
llm:
  model: "{{ $.model }}"
  kv_handle: "{{ $.kv_handle }}"
  prompt: "{{ $.new_prompt }}"
```

---

### 4. **`/lib/reasoning/stream_until@v1`ï¼ˆstableï¼‰**

```yaml
signature:
  inputs:
    - name: prompt; type: string; required: true
    - name: stop_condition; type: string; required: true
    - name: max_tokens; type: integer; default: 2048
    - name: model; type: string; required: true
  outputs:
    - name: streamed_output; type: string
version: "1.0"
stability: stable
permissions: [reasoning: stream_output]
type: llm_call
llm:
  model: "{{ $.model }}"
  prompt: "{{ $.prompt }}"
  stop_condition: "{{ $.stop_condition }}"
  max_tokens: "{{ $.max_tokens }}"
```

---

### 5. **`/lib/reasoning/speculative_decode@v1`ï¼ˆexperimentalï¼‰**

```yaml
signature:
  inputs:
    - name: prompt; type: string; required: true
    - name: draft_model; type: string; default: "phi-3-mini"
    - name: target_model; type: string; required: true
  outputs:
    - name: verified_output; type: string
    - name: acceptance_rate; type: number
version: "1.0"
stability: experimental
permissions: [reasoning: speculative_decode]
type: llm_call
llm:
  model: "{{ $.target_model }}"
  draft_model: "{{ $.draft_model }}"
  prompt: "{{ $.prompt }}"
```

> âœ… **åˆè§„ä¾æ®**ï¼š10.2 å…è®¸æ–°å¢æ¨ç†åŸè¯­ï¼›6.2 è¦æ±‚å¸¦ `signature`

---

## å››ã€æ¨ç†å¼•æ“ä¸“å±å·¥ä½œæµï¼š`/app/inference/**`

### 4.1 å¼•æ“å…¥å£ï¼š`/app/inference/native_engine_v1`

```agentic
AgenticDSL '/app/inference/native_engine_v1'
type: assign
assign:
  expr: "{{ $.llm.prompt }}"
  path: "engine_input.prompt"
next: "/app/inference/tokenize@v1"

AgenticDSL '/app/inference/tokenize@v1'
type: tool_call
tool: native_tokenize
arguments:
  text: "{{ $.engine_input.prompt }}"
output_mapping:
  tokens: "engine_state.input_ids"
next: "/app/inference/alloc_kv@v1"

AgenticDSL '/app/inference/alloc_kv@v1'
type: tool_call
tool: kv_alloc
arguments:
  num_blocks: "{{ (len($.engine_state.input_ids) + 255) // 256 }}"
output_mapping:
  block_ids: "engine_state.kv_blocks"
next: "/app/inference/run_attention@v1"

AgenticDSL '/app/inference/run_attention@v1'
type: tool_call
tool: model_step
arguments:
  tokens: "{{ $.engine_state.input_ids }}"
  kv_ref: "{{ $.engine_state.kv_blocks }}"
output_mapping:
  logits: "engine_state.logits"
  updated_kv: "engine_state.kv_blocks"
next: "{{ $.llm.output_schema ? '/app/inference/apply_grammar@v1' : '/app/inference/stream_output@v1' }}"
```

> âœ… **åˆè§„ä¾æ®**ï¼š2.1 å…è®¸ `/app/**` ä½œä¸ºçŸ¥è¯†åº”ç”¨å±‚ï¼›5.2 `tool_call` ä¸ºåˆæ³•å¶å­èŠ‚ç‚¹

---

## äº”ã€C++ æ‰§è¡ŒåŸè¯­å±‚æ¨¡å—

| C++ æ¨¡å— | å·¥å…·å | è¾“å…¥ | è¾“å‡º | æƒé™ |
|--------|--------|------|------|------|
| Tokenizer | `native_tokenize` | `{text}` | `{tokens}` | `internal: inference_core` |
| KVBlockAllocator | `kv_alloc` | `{num_blocks}` | `{block_ids}` | `internal: inference_core` |
| ModelExecutor | `model_step` | `{tokens, kv_ref}` | `{logits, updated_kv}` | `internal: inference_core` |
| GrammarCompiler | `compile_grammar` | `{schema}` | `{logits_mask}` | `internal: inference_core` |
| StreamingController | `stream_until` | `{stop_condition, max_tokens}` | `{text}` | `internal: inference_core` |

> ğŸ”’ æ‰€æœ‰å·¥å…·æƒé™ä¸º `internal`ï¼Œç¦æ­¢å¤–éƒ¨ç›´æ¥è°ƒç”¨ï¼ˆ7.2ï¼‰

---

## å…­ã€å·¥ä½œæµç¤ºä¾‹

### 6.1 åŸºç¡€ç¤ºä¾‹ï¼šæ–‡æœ¬ç”Ÿæˆ

```agentic
AgenticDSL '/main/greet'
type: assign
assign:
  expr: "Hello"
next: "/lib/reasoning/generate_text@v1"
```

### 6.2 é«˜çº§ç¤ºä¾‹ï¼šç»“æ„åŒ–ç”Ÿæˆ + KV å¤ç”¨

```agentic
AgenticDSL '/main/solve_math'
type: assign
assign:
  expr: "è§£æ–¹ç¨‹: x^2 + 2x + 1 = 0"
next: "/lib/reasoning/structured_generate@v1"

AgenticDSL '/main/explain'
type: assign
assign:
  expr: "è¯·è§£é‡Šä¸ºä»€ä¹ˆæ ¹æ˜¯ -1"
next: "/lib/reasoning/continue_from_kv@v1"
```

---

## ä¸ƒã€AgenticInfer çš„æœ¬è´¨è¶…è¶Šç‚¹

| èƒ½åŠ› | llama.cpp / vLLM / SGLang | AgenticInfer |
|------|-------------------------|-------------|
| **æ§åˆ¶ç²’åº¦** | è¯·æ±‚çº§ / Token çº§ | **DAG èŠ‚ç‚¹çº§**ï¼ˆæ¯æ­¥å¯ `assert` / Traceï¼‰ |
| **ç»„åˆæ€§** | å›ºå®š pipeline | **ä»»æ„ç»„åˆ**ï¼ˆé€šè¿‡ DAG ç¼–æ’ï¼‰ |
| **å¯éªŒè¯æ€§** | é»‘ç›’è¾“å‡º | **è¿‡ç¨‹å¯éªŒè¯**ï¼ˆ`expected_output` + Traceï¼‰ |
| **æ¼”è¿›æ€§** | æ¨¡å‹ä¸ºä¸­å¿ƒ | **å­å›¾ä¸ºä¸­å¿ƒ**ï¼ˆ`archive_to` æˆåŠŸ DAGï¼‰ |
| **ç¼“å­˜å•ä½** | Token å‰ç¼€ | **å­å›¾è¯­ä¹‰ + Token å‰ç¼€** |
| **è°ƒåº¦å•ä½** | è¯·æ±‚ batch | **DAG åˆ†æ”¯æ„ŸçŸ¥ batch** |

> ğŸš€ **æ ¸å¿ƒçªç ´**ï¼šå°†â€œæ¨ç†ç­–ç•¥â€ä» C++ ä»£ç è½¬åŒ–ä¸º DAG èŠ‚ç‚¹ï¼Œå®ç° **æ¨ç†å³ç¨‹åº**ã€‚

---

## å…«ã€åˆè§„æ€§ä¸å®‰å…¨æ€§

| è§„èŒƒè¦æ±‚ | AgenticInfer å®ç° |
|--------|------------------|
| **ä¸‰å±‚æ¶æ„** | âœ… æ— è·¨å±‚è°ƒç”¨ |
| **æ ‡å‡†åº“å¥‘çº¦** | âœ… æ‰€æœ‰ `/lib/reasoning/**` å¸¦ `signature` |
| **æƒé™æœ€å°åŒ–** | âœ… C++ å·¥å…·æƒé™ä¸º `internal` |
| **é¢„ç®—æ§åˆ¶** | âœ… åµŒå¥—å¼•æ“ç»§æ‰¿ `max_nodes * 0.8` |
| **å¯ç»ˆæ­¢æ€§** | âœ… `stream_until` å¼ºåˆ¶ `max_tokens` |
| **Trace** | âœ… æ¯æ­¥è®°å½• `reasoning_evidence` + `backend_used` |

---

## ä¹ã€æ€»ç»“

**AgenticInfer v1.2**ï¼š

- âœ… **å®Œå…¨å…¼å®¹ AgenticDSL v3.7**ï¼Œæ— éœ€ä»»ä½•è¯­ä¹‰æ‰©å±•  
- âœ… **é€šè¿‡æ ‡å‡† `/lib/reasoning/**` å­å›¾æš´éœ²èƒ½åŠ›**  
- âœ… **C++ æ¨¡å—ä»…ä½œä¸º `tool_call` å®ç°**  
- âœ… **æ¨ç†æµç¨‹ç”± `/app/inference/**` DAG ç¼–æ’**  
- âœ… **æœ¬è´¨è¶…è¶Šä¼ ç»Ÿå¼•æ“ï¼šæ¨ç†å³ DAGï¼Œç­–ç•¥å³å­å›¾**

> **æ ‡è¯­**ï¼š  
> **â€œAgenticInfer: Where Inference Becomes a Verifiable DAG.â€**
